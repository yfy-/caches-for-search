\section{Introduction}
Large-scale search engines extensively employ caching to store
pre-fetched and/or pre-computed data items, such as query result
pages, postings lists and their intersections, documents, etc., in the
main memory. Research on caching addresses various questions each
yielding alternative policies, such as which items should be accepted
into the cache (admission), which item should be removed from the
cache when it is full (eviction), whether an item should be cached
even before requested (prefetching) and when the data item in cache
should be re-fetched or -computed (refreshing) [barla, fabrizio].

Recent work on query result caching has shown that admission policies
for such a cache may be useful, especially when the cache is segmented
[] and/or results are stored in alternative formats (such as the top-k
results' URLs and snippets vs. just document identifiers) []. In this
paper, we adapt an existing admission policy, so-called TinyLFU [], to
the result caching for search engines. TinyLFU decides on the query
results to be cached using the frequency information of a large number
of previous queries (i.e., an order of magnitude larger than those in
the cache). While doing so, it employs a compact storage scheme with
Counting Bloom Filters (CBF) to store past query frequencies. Our
evaluation in this paper shows that TinyLFU admission policy coupled
with various eviction schemes improve cache hit ratios. Furthermore,
the additional space overhead for storing the metadata (i.e., past
query frequencies) is considerably low (in comparison to a plain
storage of frequencies) due to the use of CBFs.

Our findings are important because: (i) they verify the previous work
[] in that admission policies are useful for caching in web search
where query stream exhibits a power-law distribution, and (ii) they
show that improvements in hit ratios are achievable using only a small
storage space (i.e., a tiny fraction of available cache space) for
admission policy metadata. This makes tinyLFU a perfect candidate to
be used with result cache types where the space trade-off for the
cache content vs. admission metadata is strict (such as a result cache
that stores only docIDs as query results (as in []), and hence, opens
a direction for devising new hybrid caches for search engines. While
this paper presents the preliminary results demonstrating the benefits
of using TinyLFU with a traditional result cache (i.e., storing top-k
resultsâ€™ URLs and snippets) while our work in the latter direction
(with a docID cache) is in progress.

In the following section, we briefly describe TinyLFU admission policy
together with its metadata storage scheme. In Section 3, we first
describe the eviction strategies that are applied for the result cache
together with TinyLFU, and then provide details for the simulation
setup. We report our simulation results in Section 4 and provide
concluding discussions in Section 5.

%Caching tecniques are solutions that generally considered first at
%hand to boost performance of applications, especially on web. It is
%the process of keeping the items that are more likely to appear in
%the future at a faster or a closer store so that these items can be
%served within shorter times. Hence, accomodating a cache for a system
%can marginally increase the average system throughput of response
%with reasonably small costs. Concretely, as the number of items
%served from the cache increases, the system performance will also
%increase.

Whenever an item is served from the cache, we call it a cache
\textit{hit} and the opposite is called a cache
\textit{miss}. Consequently, a common metric that is used to evaluate
cache performance is the \textit{hit-rate}. It is the ratio of the
cache hits and total number of items served from the
system. Therefore, caching algorithms yielding a higher hit-rate is
preferable over others. If a caching technique can predict future
items and admit them into the cache, it is expect to obtain higher
hit-rates.

$$\mathrm{Hit\ Rate} = \frac{hit}{hit + miss}$$


Various caching methods predict future items by using statistics from
the past access patterns. Such well known ones are \textit{Least
  Recently Used LRU} and \textit{Least Frequently Used LFU} cache
eviction policies. As their name implies, if the storage for the cache
is full, at each access request for an item, the former evicts the
least recently accessed item and admits the recently requested one,
whereas the latter evicts the least frequent item. In chapter 2 we
give a detailed information of cache eviction and admission
policies. It is an important caveat to acknowledge for cache eviction
or admission policies that the overhead of making decisions about
which item to evict or admit should not overcome the benefit of
caching. In other words, using a cache should not be more expensive
than using none at all in terms of time complexity.

Cache admission or eviction policies usually require additional data
structures to be used in order to keep statistical data about past
access patterns. The storage used by these data structures are called
the meta-data of the cache. As in time complexity, the storage cost of
meta-data of the cache should not surpass the cost of the actual
cache. Otherwise, the most amount of space is kept for the meta-data
rather than the cache itself. Hence, large meta-data cost can enforce
the cache to only store a few items. Moreover, it is important to note
that cache size $\mathcal{C}$ has the largest impact on
hit-rate. Therefore, it is crucial to keep meta-data storage cost to a
minimum so that we can have a larger $\mathcal{C}.$

An example of a cache meta-data can be a frequency
histogram. \textit{LFU} eviction policy requires to have the knowledge
of frequency of the items in order to be able to decide on which item
to evict. If the frequencies are kept for all items throughout the
whole data access stream then it is called the \textit{Pure LFU
  (P-LFU)}.  However, achieving a such frequency histogram for all
items brings a blast on the meta-data size. Therefore, \textit{P-LFU}
is not applicable for majority of applications even tough it is shown
that it yields the optimal hit-rate for large enough caches
\cite{breslau1999web}.  On the contrary, \textit{LRU} does not require any
additional meta-data cost, since the cache only needs to know about
the order of accesses of items in the cache which can be efficiently
implemented using a linked list. However, \textit{LRU} only yields
competent hit-rates if the cache is large, as also what we've obtained
from our results experimentally.

Karakostas et al. \cite{karakostas2002exploitation} have offered the
usage of \textit{Window-LFU (W-LFU)} where they've shown that for Zipf
or Zipf-like distributions, keeping the frequency statistics of last
accessed $|W|$ items can approximate to \textit{P-LFU}. Therefore, the
meta-data storage cost can be reduced marginally compared to
\textit{P-LFU}. In Zipf distribution, few items are accessed very
frequently, whereas most of the items are accessed a couple of times
or even singletons. Zipf or Zipf-like distributions are common in
various domains of computer science as well as web search. Formally,
the probability of observing an item with rank $i$ is given as with
parameter $\alpha$;

$$
Pr\{i\} = \frac{1}{\mathrm{H}_{N, \alpha}i^\alpha}
$$

where $\mathrm{H}_{N, \alpha }$ is the $\mathrm{N}$th harmonic number.

Moreover, \textit{W-LFU} better adapts to changes in the access
pattern by forgetting old events. However, implementing \textit{W-LFU}
can still be costly, since the order of the last $|W|$ accesses have
to be known so that when $W + 1$th access occurs the least recent
access can be forgotten (decrementing its frequency).

In order to further reduce the meta-data cost of \textit{W-LFU},
Eingizer et al. \cite{einziger2014tinylfu} offers a highly efficient
cache admission policy \textit{Tiny LFU} that yields the best
performance when augmented with an \textit{LFU} eviction
policy. \textit{Tiny LFU} accurately approximates to \textit{W-LFU} by
exploiting the fact that data access stream has the properties of
Zipf-like distribution and the frequency counting can be effectively
approximated with the help of \textit{Bloom
  Filter}\cite{bloom1970space} theory and approximation
sketches. \textit{Tiny LFU} admission policy achieves a good
compromise between changes in distribution and as an approximation
of a frequency histogram. However, the proposed method of augmenting
\textit{Tiny LFU} with an \textit{LFU} eviction policy may stutter
in terms of hit-rate when distribution changes happen frequently.
This is due to \textit{LFU} eviction policy cannot adapt to
changes in distribution well enough.

This paper contributes the following; we offer an explanation of cache
admission and eviction policies and give a general notion
algorithm. We offer an implementation of \textit{Tiny LFU} and augment
it with various cache eviction policies as well as compare it to
\textit{Sliding Window}\cite{dimitropoulos2008eternal} approach.
Lastly we show that \textit{Tiny LFU} can perform better when
augmented with an adaptation of \textit{Greedy Dual Size Frequency}
cache eviction policy.

Section \ref{admission} describes the distinction between admission
and eviction policies and gives an algorithm to generalize their
usage. Section \ref{primer} briefly introduces \textit{Bloom Filters}
and approximation sketches. \textit{Tiny LFU} and \textit{Sliding
  window} are discussed and compared in Section \ref{tiny}. Our
experimental setup and \textit{Tiny LFU} customization is explained in
Section \ref{experiment} as well as approximate storage sizes of cache
admission and eviction policies under discussion.  Section \ref{res}
presents our results on AOL query log dataset and finally Section
\ref{conc} concludes this paper.

\section{Cache Admission and Eviction Policies}\label{admission}

\begin{algorithm}
  \caption{Generalized algorithm for cache eviction policies augmented
    with frequency based admission policies.}
  \label{alg:request}
  \begin{algorithmic}[1]
    \Function{Request}{$item$}
    \State $freq \gets \Call{admission-add}{item}$
    \State $res \gets false$
    \If{$item$ is in cache}
    \State $res \gets true$
    \State $\Call{choose-new-victim}$
    \Else
    \If{Cache is not full}
    \State Admit $item$ with new $freq$
    \State $\Call{choose-new-victim}$
    \Else
    \State $victim \gets \Call{get-victim}$
    \If{$freq \geq \Call{admission-estimate}{victim}$}
    \State Evict $victim$ admit $item$
    \State $\Call{choose-new-victim}$
    \Else
    \State Keep $victim$
    \EndIf
    \EndIf
    \EndIf
    \State \Return $res$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

In general cache eviction policy chooses a cache victim when a new
access for an item occurs (when the cache is full). For instance, this
decision can be based on \textit{LRU} or \textit{LFU} as already
discussed earlier. When used without an augmented admission policy,
the cache victim is always evicted and newly requested item is always
admitted. Conversely, the the newly requested item is only admitted to
the cache if its more popular than cache victim according to the
admission policy.  Algorithm \ref{alg:request} presents a generalized
method that uses a frequency based admission policy. In line 2 the
admission policy updates the given item's frequency and returns
it. Notice that on line 13 the ties are broken in favor of the recent
item.

\subsection{Eviction Policies Used in Experiments}
In this section we briefly describe the eviction policies that we've
experimented with.

\subsubsection{Least Frequently Used.}
Victim suggested is the item that has the least frequency. To prevent
a misconception, notice that the frequencies have to be counted with
an admission policy. The eviction policy that counts only the items in
the cache itself is called the \textit{In-Memory LFU}.

\subsubsection{Least Recently Used.}
Victim suggested is the item that is accessed least recently.

\subsubsection{Greedy Dual Size Frequency.}
\begin{equation}\label{eq:1}
  \mathcal{H}_{value}\left(i\right) = \mathcal{F}_i^{\mathcal{K}} \times
  \frac{\mathcal{C}_i}{\mathcal{S}_i} + \mathcal{L}
\end{equation}

Greedy Dual Size Frequency is proposed by \cite{arlitt2000evaluating}
that can offer a good compromise between recency and frequency that is
also cost aware. However, for our concerns the cost awareness is
ignored. Victim suggested is the item that has the lowest
          $\mathcal{H}_{value}$ which is given in \ref{eq:1}.  $\mathcal{F}_i$
is the frequency of the item, $\mathcal{C}_i$ and $\mathcal{S}_i$ are
cost and size of page of item respectively (equal to 1). Lastly,
$\mathcal{L}$ is the aging factor that is updated to the
$\mathcal{H}_{value}$ of the cache victim whenever it is evicted. This
way, newly admitted items will obtain a higher
$\mathcal{H}_{value}$. Therefore, items that are less recently
accessed has a higher chance of being the victim.

\section{Primer on Bloom Filters and Approximation Sketches}\label{primer}

\begin{figure}
\includegraphics[scale=0.6]{bloom_filter}
\caption{A simple Bloom Filter with $m$ bits and 3 hash functions
  where $k$ is the key being hashed.}
\label{fig:bf}
\end{figure}

Bloom filters allow to query the existence of an item from a set $A =
\{a_1, a_2 \cdots a_n\}$ of $n$ elements in a stream $S = \left[s_1,
  s_2, \cdots, s_l\right]$ of length $l$ where each $s_i \in A$. Bloom
filters as in Figure \ref{fig:bf} allocate a vector of $m$ bits that
are initially set to 0. When an item $s_i$ has been retrieved from the
stream $S$, $k$ distinct hash functions are applied to $s_i$ such that
obtaining positions $h_1(s_i), h_2(s_i), \cdots, h_k(s_i)$ each within
range $\left[0, m\right]$ and bits at that positions are set to 1.
Consequently, when an item $s_i$ is queried if it has seen before in
the stream, bits at positions $h_1(s_i), h_2(s_i), \cdots, h_k(s_i)$
are read and if any of them are set to 0 then $s_i$ has not seen
before for sure. The probability of obtaining a false positive is given
approximately as at \cite{fan2000summary};\newline
$$
\left( 1 - e^{-kn/m}\right)^k
$$

\subsection{Approximation Sketches}
Approximation sketches slightly differ from the Bloom filters in the
sense that they allocate $m$ counters rather than $m$ bits. Therefore,
rather than querying the previous occurrence of an item, the frequency
of the item can be approximated. There are numerous implementation of
sketches such as \textit{Count-min sketch} \cite{cormode2005improved}
and \textit{Spectral Bloom Filters} \cite{cohen2003spectral} where we
focus on \textit{Spectral Bloom Filters} with a simpler counting
scheme as \textit{Counting Bloom Filter} \cite{fan2000summary} and
\textit{Minimal Increment} \cite{cohen2003spectral} operation as in
\cite{einziger2014tinylfu}.

\subsubsection{Spectral Bloom Filter.}
Basically an \textit{SBF (Spectral Bloom Filter)} has $m$ counters and
$k$ hash functions as in bloom filters. Additionally it has two
operations namely \textit{Add} and \textit{Estimate} where the
\textit{Add} operation is used when a new item is accessed and its
frequency should be incremented.\textit{Estimate} is used to
obtain frequency approximation of an item. \textit{Estimate} works as
follows, after obtaining $k$ values at indexes by applying $k$ hash
functions, only the minimum value is returned. Assume with 4 hash
functions we obtain values $\{3,3,4,5\}$ thus the result will be
$3$. \textit{Add} operation works in a similar way with so called
\textit{Minimal Increment} method. Considering the same example only
the two values of $3$'s would be incremented. All the logic behind
using the minimum elements is that for other elements it is for sure
that a collision happened. Therefore, \textit{SBF} tries to minimize
the error by manipulating minimum elements.

\subsubsection{Saturation of Approximation Sketches.}
As already mentioned, in a Zipf distribution most of the items are
singletons i.e. occurring a single time throughout the most recent
accesses of length $W$. Due to large number of singleton items, an
approximation sketch may result in many hash collisions in a brief
time, thus resulting in poor frequency approximation for the rest of
the data stream. In such cases, it is called that the sketch is
saturated \cite{dimitropoulos2008eternal}. Therefore, to achieve
accurate approximations, admission policies that make use of
approximation sketches should deal with the saturation problem,
especially on Zipf-like distributions.


\section{Tiny LFU and Sliding Window}\label{tiny}
\subsection{Sliding Window Approach}

\textit{Sliding Window} keeps $l$ identical sketches called segments.
Identical, $l$ number of sketches are kept in a FIFO queue. At each
item access, only the sketch at the front is manipulated until
$\frac{W}{l}$ times of accesses are obtained. Afterwards, the sketch
at the front is inserted at the end of the queue and a new sketch is
fetched and all of its counters are set to 0. Therefore, frequency
about least recently accessed $\frac{W}{l}$ items are
forgotten. Notice that this method allows the sketch to avoid early
saturation, also a sliding window is obtained with the step size of
$\frac{W}{l}$.

In terms of storage, each counter at a segment can be capped to $\log
\frac{W}{l}$ since it is the maximum number that an item can
occur. However, also note that, when doing an estimation for an item,
$l$ distinct sketches should be accessed and their frequency
estimations accumulated. As a result, the estimation time will be
costly for large $l$. Therefore, we can give the total required number
of bits of the \textit{Sliding Window} approach as in Equation
\ref{eq:straw} where $n$ is the number of counters per segment;

\begin{equation}
  \label{eq:straw}
n \times l \times log_2\left(\frac{W}{l}\right)
\end{equation}

\subsection{Tiny LFU}
\textit{Tiny LFU} contains a single \textit{SBF} and a simple bloom
filter that is called the \textit{doorkeeper}. Usage of an additional
\textit{doorkeeper} might be seen as a storage overhead but its
existence allows the \textit{SBF} to have less counters. For an
accessed item, \textit{Tiny LFU} first checks its bits in the
doorkeeper, and only further increases the full counters if the item
already exists in the doorkeeper. This way, singleton items are not
reserved full counters in the \textit{SBF}. This brings 2 benefits,
firstly it slows down or avoids the saturation of the sketch due to
the fact that there will be less collisions. Secondly, since the most
of the items are singletons, we can reserve less full counters in the
\textit{SBF}. Therefore, by adding a doorkeeper, length of the
\textit{SBF} where full counters reside can be marginally decreased.

To have the feel of \textit{W-LFU}, \textit{Tiny LFU} has a novel
mechanism to keep the sketch fresh i.e. take recency into account
called the reset method. \textit{Tiny LFU} keeps a window counter
starting from $0$ and it is incremented after each access. Whenever
the window counter is equal to $W$, all of the counters in the sketch
as well as the window counter are divided by 2 and the bits in the
\textit{doorkeeper} are all set to $0$. Notice that this operation can
be obtained by right shifting bits of the counters by 1. More detail
and the justification of the reset method is presented in
\cite{einziger2014tinylfu}.

Concretely, the most important improvement that \textit{Tiny LFU}
brings is its storage optimizations, as its name also refers
to. Since, for a cache of size $C$ an item should reside in the cache
if it has a larger frequency than $1 / C$. Therefore, for a window
size of $W$, each counter can be capped to $log_2 \frac{W}{C}$ bits
since it already deserves to be in cache when its frequency hit
$\frac{W}{C}$ i.e. there is no need to further increment its
counters. For instance, if $W / C = 7$ then only 3 bits counters are
sufficient.

 Moreover, since the doorkeeper can count up to 1, full counters are
 only necessary to count up to $\frac{W}{C} - 1$. Therefore, when
 estimating a frequency, estimation from the doorkeeper and
 \textit{SBF} are added and returned. Assuming that a doorkeeper with
 $d$ bits, and full counters \textit{SBF} with length $n$ such that
 $n < d$, the total number of bits required to allocate for \textit{Tiny
   LFU} is given in Equation \ref{eq:tiny};

 \begin{equation}
   \label{eq:tiny}
d + n \times log_2 \left(\frac{W}{C} - 1\right)
\end{equation}

 \section{Experimental Setup}\label{experiment}
 \begin{figure*}
   \label{fig:zipf}
   \includegraphics{zipf_plot.png}
 \end{figure*}
 In the experiments, we use a subset of size $1$ million from the AOL
 query log.  We treat each individual query as bag of words in which
 the order will not matter between query terms. The subset has
 $562,966$ distinct queries, where in Figure \ref{fig:zipf} we
 compared its distribution to different synthetic workloads to present
 the characteristics of our query log.

\section{Results}\label{res}

\section{Conclusion}\label{conc}

\begin{table*}
\caption{Comparing different admission policies for an LRU eviction policy}
\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c |}
\hline
\multirow{2}{*}{$\mathcal{C}$} & \multicolumn{2}{c |}{\textbf{none}} &
\multicolumn{3}{c |}{\textbf{strawman}} &
\multicolumn{3}{c |}{\textbf{tiny}} & \multicolumn{2}{c |}{\textbf{pure}} \\
\cline{2-11}
& $\mathcal{H}$ & $\mathcal{M}$ & $\mathcal{H}$ & $\mathcal{M}$ & $\mathcal{W}$
& $\mathcal{H}$ & $\mathcal{M}$ &
 $\mathcal{W}$ & $\mathcal{H}$ & $\mathcal{M}$ \\ \hline
$\%1$ & 0.182 & 0 & & & & 0.206 & 50kb & 720512 & \textbf{0.236} & 11264kb \\
\hline
$\%2$ & 0.225 & 0 & & & & 0.24 & 88kb & 720512 & \textbf{0.261} & 11264kb \\
\hline
$\%5$ & 0.283 & 0 & & & & 0.291 & 165kb & 450320 & \textbf{0.297} & 11264kb \\
\hline
$\%10$ & \textbf{0.33} & 0 & & & & 0.33 & 165kb & 112580 & 0.328 & 11264kb \\
\hline
$\%20$ & 0.38 & 0 & & & & \textbf{0.381} & 330kb& 225160 & 0.369 & 11264kb \\
\hline
$\%30$ & \textbf{0.416} & 0 & & & & 0.415 & 495kb & 337740 & 0.386 & 11264kb \\
\hline
$\%40$ & \textbf{0.434} & 0 & & & & 0.433 & 660kb & 450320 & 0.408 & 11264kb \\
\hline
$\%50$ & \textbf{0.448} & 0 & & & & 0.447 & 825kb & 562900 & 0.423 & 11264kb \\
\hline
$\%60$ & 0.457 & 0 &  & & &  &  &  &  &  \\ \hline
$\%70$ & 0.463 & 0 &  & & &  &  &  &  &  \\ \hline
\end{tabular}
\end{table*}

\begin{table*}
\caption{Comparing different admission policies for an LFU eviction policy}
\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c |}
\hline
\multirow{2}{*}{$\mathcal{C}$} & \multicolumn{2}{c |}{\textbf{none}} &
\multicolumn{3}{c |}{\textbf{strawman}} &
\multicolumn{3}{c |}{\textbf{tiny}} & \multicolumn{2}{c |}{\textbf{pure}} \\
\cline{2-11}
& $\mathcal{H}$ & $\mathcal{M}$ & $\mathcal{H}$ & $\mathcal{M}$ &
$\mathcal{W}$ & $\mathcal{H}$ & $\mathcal{M}$ &
 $\mathcal{W}$ & $\mathcal{H}$ & $\mathcal{M}$ \\ \hline
$\%1$ & 0.197 & 0 & 0.214 & 110kb & 8000 & 0.227 & 28kb & 45032 &
\textbf{0.258} & 11264kb\\ \hline
$\%2$ & 0.235 & 0 & 0.251 & 308kb & 16000 & 0.264 & 55kb & 90064 &
\textbf{0.295} & 11264kb\\ \hline
$\%5$ & 0.306 & 0 & 0.309 & 660kb & 32000 & 0.326 & 193kb & 900640 &
\textbf{0.343} & 11264kb\\ \hline
$\%10$ & 0.367 & 0 & 0.361 & 1430kb & 64000 & 0.37 & 220kb & 225160 &
\textbf{0.381} & 11264kb\\ \hline
$\%20$ & 0.405 & 0 & 0.401 & 3079kb & 128000 & 0.41 & 550kb & 900640 &
\textbf{0.415} & 11264kb \\ \hline
$\%30$ & 0.429 & 0 & 0.426 & 4618kb & 128000 & \textbf{0.431} & 660kb & 675480 &
0.431 & 11264kb \\ \hline
$\%40$ & \textbf{0.443} & 0 & 0.442 & 6598kb & 256000 & 0.443 & 880kb & 900640 &
0.443 & 11264kb \\ \hline
$\%50$ & \textbf{0.452} & 0 & 0.452 & 8246kb & 256000 & 0.452 & 825kb & 562900 &
0.452 & 11264kb \\ \hline
$\%60$ & 0.458 & 0 &  & & &  &  &  &  &  \\ \hline
$\%70$ & 0.463 & 0 &  & & &  &  &  &  &  \\ \hline
\end{tabular}
\end{table*}

\begin{table*}
  \caption{Comparing different admission policies for an GDSF eviction policy
    using $4$ for $k$}
\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c |}
\hline
\multirow{2}{*}{$\mathcal{C}$} & \multicolumn{2}{c |}{\textbf{none}} &
\multicolumn{3}{c |}{\textbf{strawman}} &
\multicolumn{3}{c |}{\textbf{tiny}} & \multicolumn{2}{c |}{\textbf{pure}} \\
\cline{2-11}
& $\mathcal{H}$ & $\mathcal{M}$ & $\mathcal{H}$ & $\mathcal{M}$ &
$\mathcal{W}$ & $\mathcal{H}$ & $\mathcal{M}$ &
 $\mathcal{W}$ & $\mathcal{H}$ & $\mathcal{M}$ \\ \hline
$\%1$ & 0.241 & 0 & 0.216 & 110kb & 8000 & 0.23 & 33kb & 90064 & 0.257 &
11264kb \\ \hline
$\%2$ & 0.279 & 0 & 0.259 & 308kb & 16000 & 0.272 & 66kb & 180128 & 0.294 &
11264kb \\ \hline
$\%5$ & 0.332 & 0 & 0.32 & 660kb & 32000 & 0.333 & 138kb & 225160 & 0.342 &
11264kb \\ \hline
$\%10$ & 0.369 & 0 & 0.362 & 1540kb & 128000 & 0.376 & 275kb & 450320 & 0.379 &
11264kb \\ \hline
$\%20$ & 0.405 & 0 & 0.401 & 3079kb & 128000 & 0.412 & 550kb & 900640 & 0.413 &
11264kb \\ \hline
$\%30$ & 0.429 & 0 & 0.426 & 4618kb & 128000 & 0.431 & 660kb & 675480 & 0.431 &
11264kb \\ \hline
$\%40$ & 0.443 & 0 & 0.442 & 6598kb & 256000 & 0.443 & 880kb & 900640 & 0.443 &
11264kb \\ \hline
$\%50$ & 0.452 & 0 & 0.452 & 8246kb & 256000 & 0.452 & 825kb & 562900 & 0.452 &
11264kb \\ \hline
\end{tabular}
\end{table*}

\begin{figure}
  \includegraphics[scale=0.5]{tlfu.png}
\end{figure}
